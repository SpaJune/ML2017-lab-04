{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import random\n",
    "import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dir=\"./ml-100k/u1.base\"\n",
    "test_dir=\"./ml-100k/u1.test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#读取数据\n",
    "def readData(dir):\n",
    "    data=[]\n",
    "    with open(dir,\"r\") as file:\n",
    "        for line in file:\n",
    "            x=line\n",
    "            tuple=[int(x.split()[0]),int(x.split()[1]),int(x.split()[2])]\n",
    "            data.append(tuple)            \n",
    "        return data\n",
    "    \n",
    "#用于把数据转换为数据,没有评分的设置为0\n",
    "def getMatrix(data,user_num,item_num):\n",
    "    R=np.zeros((user_num,item_num))\n",
    "    for tuple in data:\n",
    "        R[tuple[0]-1][tuple[1]-1]=tuple[2]\n",
    "    return R\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data=readData(train_dir)\n",
    "test_data=readData(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usr_num=np.max(train_data,0)[0]\n",
    "item_num=np.max(train_data,0)[1]\n",
    "R=getMatrix(train_data,usr_num,item_num)\n",
    "R_test=getMatrix(test_data,usr_num,item_num) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(R,P,Q):   \n",
    "    usr_num=R.shape[0]\n",
    "    item_num=R.shape[1]\n",
    "    e=0\n",
    "    x=(R>0)  #大于0的代表初始矩阵已经存在的评分\n",
    "    loss=np.multiply(np.square(R-np.dot(P,Q)),x) #筛选出有评分的 \n",
    "    loss=np.sum(loss) #计算loss总和\n",
    "    total=np.sum(x)\n",
    "    return loss\n",
    "\n",
    "\n",
    "#lamda：正则项\n",
    "def matrix_factorization(R,K,step,lamda,validation_data):\n",
    "    usr_num=R.shape[0]\n",
    "    item_num=R.shape[1]\n",
    "    P=np.random.rand(usr_num,K)\n",
    "    Q=np.random.rand(K,item_num)\n",
    "    last_loss=loss(validation_data,P,Q)\n",
    "    validation_loss=[last_loss]\n",
    "    for s in range(step):\n",
    "        '''\n",
    "        for i in range(usr_num):\n",
    "            tmp=Q                 #对P的每一行令偏导为0，求出闭式解\n",
    "            inv=np.linalg.inv(np.dot(tmp,np.transpose(tmp))+lamda*np.eye(K))\n",
    "            mul=np.dot(R[i,:],np.transpose(Q))\n",
    "            P[i,:]=np.dot(mul,inv)\n",
    "        for j in range(item_num):\n",
    "            tmp=P              #对Q的每一列令偏导为0，求出闭式解\n",
    "            inv=np.linalg.inv(np.dot(np.transpose(tmp),tmp)+lamda*np.eye(K))\n",
    "            mul=np.dot(inv,np.transpose(tmp))\n",
    "            Q[:,j]=np.dot(mul,R[:,j])      \n",
    "            '''\n",
    "        #print(s)\n",
    "        for u in range(usr_num):\n",
    "            sum1=0\n",
    "            sum2=0\n",
    "            for i in range(item_num):\n",
    "                if(R[u,i]!=0):  #求闭式解，闭式解分为两部分求和，最后两个做矩阵乘法\n",
    "                    tmp=Q[:,i].reshape((K,1))\n",
    "                    sum1+=R[u,i]*np.transpose(tmp)                   \n",
    "                    sum2+=np.dot(tmp,np.transpose(tmp))\n",
    "            if(type(sum1) is type(0)): #如果sum1==0，证明这一列没有评分，则过滤掉\n",
    "                continue\n",
    "            sum2+=lamda*np.eye(K)\n",
    "            P[u,:]=np.dot(sum1,np.linalg.inv(sum2)).reshape((K))\n",
    "            \n",
    "        for i in range(item_num):\n",
    "            sum1=0\n",
    "            sum2=0\n",
    "            for u in range(usr_num):\n",
    "                if(R[u,i]!=0):  #求闭式解\n",
    "                    tmp=P[u,:].reshape((1,K))\n",
    "                    sum1+=R[u,i]*np.transpose(tmp)\n",
    "                    sum2+=np.dot(np.transpose(tmp),tmp)\n",
    "            if(type(sum1) is type(0)): #如果sum1==0\n",
    "                continue\n",
    "            sum2+=lamda*np.eye(K)\n",
    "            \n",
    "            Q[:,i]=np.dot(np.linalg.inv(sum2),sum1).reshape((K))\n",
    "            \n",
    "        #loss不带正则项，如果要看带正则项的，可以加上注释那段\n",
    "        loss_val=loss(validation_data,P,Q) #np.sum(np.square(Q))*lamda/2+np.sum(np.square(P))*lamda/2\n",
    "        if(abs(loss_val-last_loss)<0.00001):\n",
    "            return P,Q,validation_loss\n",
    "        last_loss=loss_val\n",
    "        validation_loss.append(loss_val)\n",
    "        \n",
    "        #print(loss_val)\n",
    "    return P,Q,validation_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#以下的R是测试算法准确性时候用的\n",
    "'''\n",
    "R = [\n",
    "     [5,9,0,1],\n",
    "     [4,0,0,1],\n",
    "     [7,1,0,5],\n",
    "     [1,0,3,4],\n",
    "     [0,1,5,4],\n",
    "    ]\n",
    "R=np.array(R)\n",
    "R_test=R\n",
    "'''\n",
    "K=2\n",
    "lamda=0.02\n",
    "step=200\n",
    "\n",
    "P,Q,validation_loss=matrix_factorization(R,K,step,lamda,R_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_factorization_stochastic(R,alpha,K,step,lamda,validation_data):\n",
    "    usr_num=R.shape[0]\n",
    "    item_num=R.shape[1]\n",
    "    P=np.random.rand(usr_num,K)  #初始化\n",
    "    Q=np.random.rand(K,item_num) #初始化\n",
    "    last_loss=loss(validation_data,P,Q)\n",
    "    validation_loss=[last_loss]\n",
    "    record_size=len(train_data)\n",
    "    for i in range(step):\n",
    "        sample_index=random.randint(0,record_size-1)\n",
    "        user=train_data[sample_index][0]-1\n",
    "        item=train_data[sample_index][1]-1\n",
    "        sample=train_data[sample_index][2]\n",
    "        '''\n",
    "        while(sample==0):   #找到一个不等于0的\n",
    "            user=random.randint(0,usr_num-1)\n",
    "            item=random.randint(0,item_num-1)\n",
    "            sample=R[user,item]\n",
    "        '''\n",
    "        e=R[user,item]-np.dot(P[user,:],Q[:,item])\n",
    "        P[user,:]=P[user,:]+2*alpha*e*np.transpose(Q[:,item])-alpha*lamda*P[user,:]\n",
    "        Q[:,item]=Q[:,item]+2*alpha*e*np.transpose(P[user,:])-alpha*lamda*Q[:,item]\n",
    "        '''\n",
    "        for k in range(K):                  #正常做法       \n",
    "            P[i,k]=P[i,k]+2*alpha*e*Q[k,j]-alpha*lamda*P[i,k]  \n",
    "            Q[k,j]=Q[k,j]+2*alpha*e*P[i,k]-alpha*lamda*Q[k,j]\n",
    "         ''' \n",
    "        loss_val=loss(validation_data,P,Q)+np.sum(np.square(Q))*lamda/2+np.sum(np.square(P))*lamda/2\n",
    "        validation_loss.append(loss_val)\n",
    "        if(abs(loss_val-last_loss)<0.00001):\n",
    "            return P,Q,validation_loss\n",
    "        last_loss=loss_val\n",
    "        #print(loss_val)\n",
    "    return P,Q,validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "R = [\n",
    "     [5,3,0,1],\n",
    "     [4,0,0,1],\n",
    "     [1,1,0,5],\n",
    "     [1,0,0,4],\n",
    "     [0,1,5,4],\n",
    "    ]\n",
    "R=np.array(R)\n",
    "R_test=R\n",
    "'''\n",
    "alpha=0.01\n",
    "K=2\n",
    "lamda=0.01\n",
    "step=50000\n",
    "\n",
    "P_g,Q_g,validation_loss_g=matrix_factorization_stochastic(R,alpha,K,step,lamda,R_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#梯度下降所画的图\n",
    "plt.figure()\n",
    "iter=range(len(validation_loss_g))\n",
    "plt.plot(iter,validation_loss_g)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.419337    0.41363577  0.65867067 ...,  0.68892002  0.34952414\n",
      "   0.36523869]\n",
      " [ 0.18508678  0.10124894  0.27935674 ...,  0.77727799  0.39005088\n",
      "   0.16387575]\n",
      " [ 0.25105759  0.28435116  0.39947782 ...,  0.19886287  0.10283496\n",
      "   0.21746509]\n",
      " ..., \n",
      " [ 0.17894194  0.12508392  0.27388364 ...,  0.59321871  0.29824971\n",
      "   0.15754323]\n",
      " [ 0.34650271  0.40544797  0.55316453 ...,  0.19885164  0.10425483\n",
      "   0.29971314]\n",
      " [ 0.46188944  0.44366251  0.72383966 ...,  0.82834813  0.41963112\n",
      "   0.40269327]]\n",
      "[[ 5.  3.  4. ...,  0.  0.  0.]\n",
      " [ 4.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 5.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  5.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#梯度下降方法所求出来的P和Q\n",
    "print(np.dot(P_g,Q_g))\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEKCAYAAADEovgeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nK52TjZmZle7/A8bGSKFuY5OzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19eb0f3e940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.88571741  3.00053738  2.97354516 ...,  2.18329907  3.60868344\n",
      "   3.22991334]\n",
      " [ 3.95687672  3.07362324  3.03401564 ...,  2.21025863  3.65783312\n",
      "   3.26489922]\n",
      " [ 3.32499517  2.47893366  2.5150595  ...,  1.93187277  3.17068803\n",
      "   2.88188836]\n",
      " ..., \n",
      " [ 4.29874098  3.43389931  3.32756553 ...,  2.33320328  3.88540837\n",
      "   3.42077987]\n",
      " [ 4.55975796  3.9914358   3.64538363 ...,  2.2242495   3.79540058\n",
      "   3.16347417]\n",
      " [ 3.89846106  3.14148278  3.02678333 ...,  2.09631892  3.49809241\n",
      "   3.0658361 ]]\n",
      "[[ 5.  3.  4. ...,  0.  0.  0.]\n",
      " [ 4.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 5.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  5.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#als算法运算之后所得到的loss图和PQ的积\n",
    "plt.figure()\n",
    "iter=range(len(validation_loss))\n",
    "plt.plot(iter,validation_loss)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "print(np.dot(P,Q))\n",
    "print(R)\n",
    "#error(R,P,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
